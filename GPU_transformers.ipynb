{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT4All-j 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary c:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: C:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2 did not contain cudart64_110.dll as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: C:\\Windows\\System32\\lxss\\lib did not contain cudart64_110.dll as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Program Files (x86)/Razer/ChromaBroadcast/bin'), WindowsPath('C:/WINDOWS/system32/config/systemprofile/AppData/Local/Microsoft/WindowsApps'), WindowsPath('C:/Users/Aviv9/miniconda3/envs/LLMBots2/Library/mingw-w64/bin'), WindowsPath('C:/Program Files/JetBrains/DataSpell 2021.3.2/bin'), WindowsPath('C:/ProgramData/DockerDesktop/version-bin'), WindowsPath('C:/Program Files/Razer Chroma SDK/bin'), WindowsPath('C:/Program Files/Razer/ChromaBroadcast/bin'), WindowsPath('C:/Program Files (x86)/Razer Chroma SDK/bin'), WindowsPath('C:/AIEnhancer/onnx_gen_video')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef79d8e54d474bf8a0f6d73444ee8c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "device_map = {\n",
    "    \"transformer.word_embeddings\": 0,\n",
    "    \"transformer.word_embeddings_layernorm\": 0,\n",
    "    \"lm_head\": 0,\n",
    "    \"transformer.h\": 0,\n",
    "    \"transformer.ln_f\": 0,\n",
    "    \"transformer.wte.weight\":0,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/gpt4all-j\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"nomic-ai/gpt4all-j\",\n",
    "        revision=\"v1.3-groovy\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quantization_config,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPT-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPT-7B test\n",
    "# pip install einops\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "# from accelerate import infer_auto_device_map\n",
    "# device_map = infer_auto_device_map(model, max_memory={0: \"9.5GiB\", \"cpu\": \"30GiB\"})\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "quantization_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "\n",
    "device_map = {\n",
    "    'transformer.wte': 0,\n",
    "    'transformer.emb_drop': 0,\n",
    "    'transformer.blocks.0': 0,\n",
    "    'transformer.blocks.1': 0,\n",
    "    'transformer.blocks.2': 0,\n",
    "    'transformer.blocks.3': 0,\n",
    "    'transformer.blocks.4': 0,\n",
    "    'transformer.blocks.5': 0,\n",
    "    'transformer.blocks.6': 0,\n",
    "    'transformer.blocks.7': 0,\n",
    "    'transformer.blocks.8': 0,\n",
    "    'transformer.blocks.9': 0,\n",
    "    'transformer.blocks.10': 0,\n",
    "    'transformer.blocks.11': 0,\n",
    "    'transformer.blocks.12': 0,\n",
    "    'transformer.blocks.13': 0,\n",
    "    'transformer.blocks.14': 0,\n",
    "    'transformer.blocks.15': 0,\n",
    "    'transformer.blocks.16': 0,\n",
    "    'transformer.blocks.17': 0,\n",
    "    'transformer.blocks.18': 0,\n",
    "    'transformer.blocks.19': 0,\n",
    "    'transformer.blocks.20': 0,\n",
    "    'transformer.blocks.21': 0,\n",
    "    'transformer.blocks.22': 0,\n",
    "    'transformer.blocks.23': 0,\n",
    "    'transformer.blocks.24': 0,\n",
    "    'transformer.blocks.25.norm_1': 0,\n",
    "    'transformer.blocks.25.attn.Wqkv': 0,\n",
    "    'transformer.blocks.25.attn.out_proj': 0,\n",
    "    'transformer.blocks.25.norm_2': 0,\n",
    "    'transformer.blocks.25.ffn': 0,\n",
    "    'transformer.blocks.25.resid_attn_dropout': 0,\n",
    "    'transformer.blocks.25.resid_ffn_dropout': 0,\n",
    "    'transformer.blocks.26': 0,\n",
    "    'transformer.blocks.27': 0,\n",
    "    'transformer.blocks.28': 0,\n",
    "    'transformer.blocks.29': 0,\n",
    "    'transformer.blocks.30': 0,\n",
    "    'transformer.blocks.31': 0,\n",
    "    'transformer.norm_f': 0\n",
    "}\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    'mosaicml/mpt-7b',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=bfloat16,\n",
    "    max_seq_len=2048,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "print(\"model loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPT-7B stopping cretiria\n",
    "\n",
    "import torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# mtp-7b is trained to add \"<|endoftext|>\" at the end of generations\n",
    "stop_token_ids = tokenizer.convert_tokens_to_ids([\"<|endoftext|>\"])\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # device=device,\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model will ramble\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    top_p=0.15,  # select from top tokens whose probability add up to 15%\n",
    "    top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n",
    "    max_new_tokens=64,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vicuna-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=torch.bfloat16 with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary c:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: C:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2 did not contain cudart64_110.dll as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: C:\\Windows\\System32\\lxss\\lib did not contain cudart64_110.dll as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "c:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Program Files/Razer/ChromaBroadcast/bin'), WindowsPath('C:/Program Files (x86)/Razer/ChromaBroadcast/bin'), WindowsPath('C:/WINDOWS/system32/config/systemprofile/AppData/Local/Microsoft/WindowsApps'), WindowsPath('C:/Users/Aviv9/miniconda3/envs/LLMBots2/Library/mingw-w64/bin'), WindowsPath('C:/Program Files/Razer Chroma SDK/bin'), WindowsPath('C:/Program Files/JetBrains/DataSpell 2021.3.2/bin'), WindowsPath('C:/AIEnhancer/onnx_gen_video'), WindowsPath('C:/Program Files (x86)/Razer Chroma SDK/bin'), WindowsPath('C:/ProgramData/DockerDesktop/version-bin')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7d105c340241fbaaeac6dca079ba4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# Vicuna-7b test\n",
    "# pip install sentencepiece protobuf<=3.20.0\n",
    "# pip install git+https://github.com/zphang/transformers.git@llama_push\n",
    "from torch import cuda, bfloat16\n",
    "from accelerate import infer_auto_device_map\n",
    "import transformers\n",
    "\n",
    "# device_map = infer_auto_device_map(model, max_memory={0: \"10GiB\", \"cpu\": \"30GiB\"})\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "quantization_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 0,\n",
    "    'model.layers.15': 0,\n",
    "    'model.layers.16': 0,\n",
    "    'model.layers.17': 0,\n",
    "    'model.layers.18': 0,\n",
    "    'model.layers.19': 0,\n",
    "    'model.layers.20': 0,\n",
    "    'model.layers.21': 0,\n",
    "    'model.layers.22': 0,\n",
    "    'model.layers.23': 0,\n",
    "    'model.layers.24': 0,\n",
    "    'model.layers.25.self_attn.q_proj': 0,\n",
    "    'model.layers.25.self_attn.k_proj': 0,\n",
    "    'model.layers.25.self_attn.v_proj': 0,\n",
    "    'model.layers.25.self_attn.o_proj': 0,\n",
    "    'model.layers.25.self_attn.rotary_emb': 0,\n",
    "    'model.layers.25.mlp': 0,\n",
    "    'model.layers.25.input_layernorm': 0,\n",
    "    'model.layers.25.post_attention_layernorm': 0,\n",
    "    'model.layers.26': 0,\n",
    "    'model.layers.27': 0,\n",
    "    'model.layers.28': 0,\n",
    "    'model.layers.29': 0,\n",
    "    'model.layers.30': 0,\n",
    "    'model.layers.31': 0,\n",
    "    'model.norm': 0,\n",
    "    'lm_head': 0\n",
    "}\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    'Tribbiani/vicuna-7b',\n",
    "    torch_dtype=bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"model loaded\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Tribbiani/vicuna-7b\")\n",
    "print(\"tokenizer loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task list creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following questions as best you can.\n",
    "You can interact with a computer program that can give you access to the following tools:\n",
    "\n",
    "Current Search: useful for when you need to answer questions about current events or the current state of the world\n",
    "\n",
    "The way you use the tools is by specifying a json blob that will be referred as $JSON_BLOB.\n",
    "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the tools input).\n",
    "\n",
    "The only values that should be in the \"action\" field are: Current Search\n",
    "\n",
    "The $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"action\": $TOOL_NAME,\n",
    "    \"action_input\": $INPUT\n",
    "}\n",
    "```\n",
    "You would be given a task as follows:\n",
    "Question: <the input question you must answer>.\n",
    "\n",
    "You should always answer step by step in the following format:\n",
    "Thought: <think about the question and how you are going to answer it>\n",
    "Action: <specify an action from the tools you can access>\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Result: <the result of your action>\n",
    "Observation: <make an observation based on the action Results>.\n",
    "... (this Thought/Action/Result/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: <the final answer to the original input question>\n",
    "\n",
    "Begin! Reminder to always use the exact characters `Final Answer` when responding.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "templateTaskCreation =  rf\"\"\"\n",
    "You are an AI who creates a precise task list to accomplish a certain objective. Each task should start with a number of the task and should end with '.\\n'. The task list should be short and contain MAXIMUM 10 tasks. Each task should start with a specific action.  Don't write anything other than the tasks.\n",
    "Objective: Plan a 5 days trip to Madrid, include historical locations and most recommended low budget restaurants, you should also create itinerary for the trip.\n",
    "Assistant: let's think step by step:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 396], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Human\" in tokenizer.decode([835, 12968])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers    \n",
    "\n",
    "class StopOnTokens(transformers.StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "       stop_tokens = [2277, 29937, 12968, 29901]\n",
    "    #    print(tokenizer.decode(input_ids[0][-1]), tokenizer.decode(input_ids[0][-2]))\n",
    "       if (list(input_ids[0][-4:]) == stop_tokens):\n",
    "           return True \n",
    "       return False\n",
    "\n",
    "device = torch.cuda.is_available()\n",
    "\n",
    "message = templateTaskCreation\n",
    "inputs = tokenizer(message, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 512,\n",
    "    temperature = 0.1,\n",
    "    top_k = 0,\n",
    "    top_p = 0.15,\n",
    "    repetition_penalty = 1.1,\n",
    "    stopping_criteria=transformers.StoppingCriteriaList([StopOnTokens()]),\n",
    "    # streamer = transformers.TextStreamer(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "1. Research the best historical locations in Madrid\n",
      "2. Find the most recommended low budget restaurants in Madrid\n",
      "3. Create an itinerary for the trip including the historical locations and restaurants\n",
      "4. Organize transportation for the trip\n",
      "5. Book accommodations for the trip\n",
      "6. Purchase tickets for any attractions or tours\n",
      "7. Pack for the trip\n",
      "8. Inform family members about the plan\n",
      "9. Print out the itinerary and packing list\n",
      "10. Double check all arrangements before departing\n",
      "\n",
      "Please let me know if there is something else I can help you with.\n",
      "### Human:\n"
     ]
    }
   ],
   "source": [
    "completion_tokens = tokens[0][inputs['input_ids'].size(1):]\n",
    "completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokens[0][-4:]) == [2277, 29937, 12968, 29901]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ### Human:'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2277, 29937, 12968, 29901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Research historical locations in Madrid\n",
      " Find low budget restaurants in Madrid\n",
      " Create an itinerary for the trip\n",
      " Plan transportation options\n",
      " Finalize the plan\n",
      " Review and finalize the plan\n",
      " Print the plan\n",
      " Share the plan with family members or friends\n",
      " Enjoy your trip!\n"
     ]
    }
   ],
   "source": [
    "def parse_tasks(completion):\n",
    "    return [task[2:] for task in completion.split('\\n') if task != '']\n",
    "\n",
    "tasks = parse_tasks(completion)\n",
    "print(*tasks, sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool chooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_action_chooser =  r\"\"\"\n",
    "You are an AI who uses tools in order to complete your Objective.\n",
    "You can only use exactly ONE tool.\n",
    "The tool you are planning to must be one of the following tools:\n",
    "1. WRITE_FILE(content)\n",
    "2. READ_FILE(path)\n",
    "3. RUN_PYTHON(code)\n",
    "4. SEARCH_ONLINE(query)\n",
    "5. EXTRACT_INFO(url, the information to look for)\n",
    "6. NO_TOOL\n",
    "\n",
    "Answer in JSON format with the following keys:\n",
    "tool_name, tool_input.\n",
    "Do not write anything other than the chosen tool.\n",
    "Assistant: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "\n",
      "\n",
      "CONSTRAINTS:\n",
      "1. No user assistance.\n",
      "2. Cannot run Python code that requires user input.\n",
      "\n",
      "\n",
      "ACTIONS:\n",
      "\n",
      "1. \"READ_FILE\": read the current state of a file. The schema for the action is:\n",
      "\n",
      "READ_FILE: <PATH>\n",
      "\n",
      "2. \"WRITE_FILE\": write a block of text to a file. The schema for the action is:\n",
      "\n",
      "WRITE_FILE: <PATH>\n",
      "```\n",
      "<TEXT>\n",
      "```\n",
      "\n",
      "3. \"RUN_PYTHON\": run a Python file. The schema for the action is:\n",
      "\n",
      "RUN_PYTHON: <PATH>\n",
      "\n",
      "4. \"SEARCH_ONLINE\": search online and get back a list of URLs relevant to the query. The schema for the action is:\n",
      "\n",
      "SEARCH_ONLINE: <QUERY>\n",
      "\n",
      "5. EXTRACT_INFO: extract specific information from a webpage. The schema for the action is:\n",
      "\n",
      "EXTRACT_INFO: <URL>, <a brief instruction to GPT for information to extract>\n",
      "\n",
      "6. \"SHUTDOWN\": shut down the program. The schema for the action is:\n",
      "\n",
      "SHUTDOWN\n",
      "\n",
      "\n",
      "RESOURCES:\n",
      "1. File contents after reading file.\n",
      "2. Online search results returning URLs.\n",
      "3. Output of running a Python file.\n",
      "\n",
      "\n",
      "PERFORMANCE EVALUATION:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities. \n",
      "2. Constructively self-criticize your big-picture behaviour constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every action has a cost, so be smart and efficent. Aim to complete tasks in the least number of steps.\n",
      "\n",
      "\n",
      "Write only one action. The action must one of the actions specified above and must be written according to the schema specified above.\n",
      "\n",
      "After the action, also write the following metadata JSON object, which must be parsable by Python's json.loads():\n",
      "{\n",
      "    \"criticism\": \"<constructive self-criticism of actions performed so far, if any>\",\n",
      "    \"reason\": \"<a sentence explaining the action above>\",\n",
      "    \"plan\": \"<a short high-level plan in plain English>\",\n",
      "    \"speak\": \"<a short summary of thoughts to say to the user>\"\n",
      "}\n",
      "\n",
      "If you want to run an action that is not in the above list of actions, send the SHUTDOWN action instead and explain which action you wanted to run in the metadata JSON object.\n",
      "So, write one action and one metadata JSON object, nothing else.\n",
      "Plan a 5 days trip to Madrid, include historical locations and most recommended low budget restaurants, you should also create itinerary for the trip.\n",
      "Determine which next action to use, and write one valid action, a newline, and one valid metadata JSON object, both according to the specified schema:\n",
      "```\n",
      "\"READ_FILE\": \"C:/Users/UserName/Documents/file.txt\",\n",
      "\"WRITE_FILE\": "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m message \u001b[39m=\u001b[39m template_action_chooser\n\u001b[0;32m     73\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(message, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m     75\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs,\n\u001b[0;32m     76\u001b[0m     max_new_tokens \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m,\n\u001b[0;32m     77\u001b[0m     temperature \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m,\n\u001b[0;32m     78\u001b[0m     \u001b[39m# top_k=0,\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[39m# top_p=0.5,\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     repetition_penalty \u001b[39m=\u001b[39m \u001b[39m1.05\u001b[39m,\n\u001b[0;32m     81\u001b[0m     streamer \u001b[39m=\u001b[39m TextStreamer(tokenizer\u001b[39m=\u001b[39mtokenizer),\n\u001b[0;32m     82\u001b[0m     \u001b[39m# stopping_criteria = stopping_criteria\u001b[39;00m\n\u001b[0;32m     83\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\transformers\\generation\\utils.py:1437\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1432\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1433\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1434\u001b[0m         )\n\u001b[0;32m   1436\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgreedy_search(\n\u001b[0;32m   1438\u001b[0m         input_ids,\n\u001b[0;32m   1439\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1440\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1441\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1442\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1443\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1444\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1445\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1446\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[0;32m   1447\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1448\u001b[0m     )\n\u001b[0;32m   1450\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[0;32m   1451\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\transformers\\generation\\utils.py:2248\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2245\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2247\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2248\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2249\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2250\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2251\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2252\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2253\u001b[0m )\n\u001b[0;32m   2255\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2256\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:852\u001b[0m, in \u001b[0;36mGPTJForCausalLM.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m    847\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    850\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> 852\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    853\u001b[0m     input_ids,\n\u001b[0;32m    854\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    855\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    856\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    857\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    858\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    859\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    860\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    861\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    862\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    863\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    864\u001b[0m )\n\u001b[0;32m    865\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    867\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:687\u001b[0m, in \u001b[0;36mGPTJModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    678\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    679\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    680\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    684\u001b[0m         head_mask[i],\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    688\u001b[0m         hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    689\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    690\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    691\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    692\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    693\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    694\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    695\u001b[0m     )\n\u001b[0;32m    697\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    698\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:308\u001b[0m, in \u001b[0;36mGPTJBlock.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    306\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    307\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 308\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    309\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    310\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    311\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    312\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    313\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    314\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    315\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    316\u001b[0m )\n\u001b[0;32m    317\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    318\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:256\u001b[0m, in \u001b[0;36mGPTJAttention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    253\u001b[0m     present \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[39m# compute self-attention: V x Softmax(QK^T)\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    258\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m    259\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj(attn_output)\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\miniconda3\\envs\\LLMBots2\\lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:165\u001b[0m, in \u001b[0;36mGPTJAttention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    162\u001b[0m mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfinfo(attn_weights\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin\n\u001b[0;32m    163\u001b[0m \u001b[39m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(mask_value, dtype\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mto(attn_weights\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    166\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(causal_mask, attn_weights, mask_value)\n\u001b[0;32m    168\u001b[0m attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer    \n",
    "\n",
    "template_action_chooser =  r\"\"\"\n",
    "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
    "\n",
    "\n",
    "CONSTRAINTS:\n",
    "1. No user assistance.\n",
    "2. Cannot run Python code that requires user input.\n",
    "\n",
    "\n",
    "ACTIONS:\n",
    "\n",
    "1. \"READ_FILE\": read the current state of a file. The schema for the action is:\n",
    "\n",
    "READ_FILE: <PATH>\n",
    "\n",
    "2. \"WRITE_FILE\": write a block of text to a file. The schema for the action is:\n",
    "\n",
    "WRITE_FILE: <PATH>\n",
    "```\n",
    "<TEXT>\n",
    "```\n",
    "\n",
    "3. \"RUN_PYTHON\": run a Python file. The schema for the action is:\n",
    "\n",
    "RUN_PYTHON: <PATH>\n",
    "\n",
    "4. \"SEARCH_ONLINE\": search online and get back a list of URLs relevant to the query. The schema for the action is:\n",
    "\n",
    "SEARCH_ONLINE: <QUERY>\n",
    "\n",
    "5. EXTRACT_INFO: extract specific information from a webpage. The schema for the action is:\n",
    "\n",
    "EXTRACT_INFO: <URL>, <a brief instruction to GPT for information to extract>\n",
    "\n",
    "6. \"SHUTDOWN\": shut down the program. The schema for the action is:\n",
    "\n",
    "SHUTDOWN\n",
    "\n",
    "\n",
    "RESOURCES:\n",
    "1. File contents after reading file.\n",
    "2. Online search results returning URLs.\n",
    "3. Output of running a Python file.\n",
    "\n",
    "\n",
    "PERFORMANCE EVALUATION:\n",
    "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities. \n",
    "2. Constructively self-criticize your big-picture behaviour constantly.\n",
    "3. Reflect on past decisions and strategies to refine your approach.\n",
    "4. Every action has a cost, so be smart and efficent. Aim to complete tasks in the least number of steps.\n",
    "\n",
    "\n",
    "Write only one action. The action must one of the actions specified above and must be written according to the schema specified above.\n",
    "\n",
    "After the action, also write the following metadata JSON object, which must be parsable by Python's json.loads():\n",
    "{\n",
    "    \"criticism\": \"<constructive self-criticism of actions performed so far, if any>\",\n",
    "    \"reason\": \"<a sentence explaining the action above>\",\n",
    "    \"plan\": \"<a short high-level plan in plain English>\",\n",
    "    \"speak\": \"<a short summary of thoughts to say to the user>\"\n",
    "}\n",
    "\n",
    "If you want to run an action that is not in the above list of actions, send the SHUTDOWN action instead and explain which action you wanted to run in the metadata JSON object.\n",
    "So, write one action and one metadata JSON object, nothing else.\n",
    "Plan a 5 days trip to Madrid, include historical locations and most recommended low budget restaurants, you should also create itinerary for the trip.\n",
    "Determine which next action to use, and write one valid action, a newline, and one valid metadata JSON object, both according to the specified schema:\n",
    "\"\"\"\n",
    "\n",
    "device = torch.cuda.is_available()\n",
    "message = template_action_chooser\n",
    "inputs = tokenizer(message, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 512,\n",
    "    temperature = 0.1,\n",
    "    # top_k=0,\n",
    "    # top_p=0.5,\n",
    "    repetition_penalty = 1.05,\n",
    "    streamer = TextStreamer(tokenizer=tokenizer),\n",
    "    # stopping_criteria = stopping_criteria\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation args\n",
    "max_new_tokens = 1024\n",
    "temperature = 0.5\n",
    "top_k = 5\n",
    "top_p = 0.9\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    tokenizer=tokenizer,\n",
    "    model=model, \n",
    "    max_length=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k = top_k,\n",
    "    top_p = top_p,\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the following questions as best you can.\n",
    "You can interact with a computer program that can give you access to the following tools:\n",
    "\n",
    "Current Search: useful for when you need to answer questions about current events or the current state of the world\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the tools input).\n",
    "\n",
    "The only values that should be in the \"action\" field are: Current Search\n",
    "\n",
    "The $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"action\": $TOOL_NAME,\n",
    "    \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "You would be given a task as follows:\n",
    "Question: the input question you must answer.\n",
    "\n",
    "You should always answer in the following format:\n",
    "Thought: think about the question and how you are going to answer it\n",
    "Action: (optional)\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "After requesting action you should STOP and wait for the computer program to return the action result.\n",
    "Result: <the result of your action>\n",
    "Observation: you should make an observation based on the action Results.\n",
    "... (this Thought/Action/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Reminder to always use the exact characters `Final Answer` when responding.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\"], \n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_llm, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True)\n",
    "# question = \"How many cities are in london?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append(\"Question: Plan a 5 days trip to Madrid, include historical locations and most recommended low budget restaurants, you should also create itinerary for the trip.\")\n",
    "answer = llm_chain.predict(human_input=\"\\n\".join(chat_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_chain.predict(human_input=\"ls ~\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_chain.predict(human_input=\"Can you list the last 10 presidents of the united states?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "search = GoogleSearchAPIWrapper(\n",
    "    google_api_key=\"AIzaSyAP55t5T-MpdccsquOW17a4BQSZBqPbWIM\",\n",
    "    google_cse_id=\"2361e668410524176\"\n",
    ")\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Current Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events or the current state of the world\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "agent_chain = initialize_agent(tools, local_llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"list the last 10 presidents of the united states\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robo-GPT-QaSuWWH8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
