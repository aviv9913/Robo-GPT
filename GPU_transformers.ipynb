{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 7.75MB/s]\n",
      "c:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aviv9\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.00/2.00 [00:00<?, ?B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 141/141 [00:00<00:00, 141kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 427/427 [00:00<00:00, 428kB/s]\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 25.5k/25.5k [00:00<00:00, 25.5MB/s]\n",
      "Downloading (…)l-00001-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 62.3MB/s]\n",
      "Downloading (…)l-00002-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.0MB/s]\n",
      "Downloading (…)l-00003-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.0MB/s]\n",
      "Downloading (…)l-00004-of-00033.bin: 100%|██████████| 405M/405M [00:07<00:00, 56.8MB/s]\n",
      "Downloading (…)l-00005-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.4MB/s]\n",
      "Downloading (…)l-00006-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 62.8MB/s]\n",
      "Downloading (…)l-00007-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 62.5MB/s]\n",
      "Downloading (…)l-00008-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.2MB/s]\n",
      "Downloading (…)l-00009-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.4MB/s]\n",
      "Downloading (…)l-00010-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.0MB/s]\n",
      "Downloading (…)l-00011-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 62.9MB/s]\n",
      "Downloading (…)l-00012-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.2MB/s]\n",
      "Downloading (…)l-00013-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 61.1MB/s]\n",
      "Downloading (…)l-00014-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.2MB/s]\n",
      "Downloading (…)l-00015-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 61.5MB/s]\n",
      "Downloading (…)l-00016-of-00033.bin: 100%|██████████| 405M/405M [00:07<00:00, 53.4MB/s]\n",
      "Downloading (…)l-00017-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.3MB/s]\n",
      "Downloading (…)l-00018-of-00033.bin: 100%|██████████| 405M/405M [00:07<00:00, 57.1MB/s]\n",
      "Downloading (…)l-00019-of-00033.bin: 100%|██████████| 405M/405M [00:07<00:00, 54.0MB/s]\n",
      "Downloading (…)l-00020-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 58.4MB/s]\n",
      "Downloading (…)l-00021-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 61.0MB/s]\n",
      "Downloading (…)l-00022-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 59.2MB/s]\n",
      "Downloading (…)l-00023-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 61.6MB/s]\n",
      "Downloading (…)l-00024-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 63.0MB/s]\n",
      "Downloading (…)l-00025-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 60.0MB/s]\n",
      "Downloading (…)l-00026-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 59.4MB/s]\n",
      "Downloading (…)l-00027-of-00033.bin: 100%|██████████| 405M/405M [00:07<00:00, 54.1MB/s]\n",
      "Downloading (…)l-00028-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 60.6MB/s]\n",
      "Downloading (…)l-00029-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 61.0MB/s]\n",
      "Downloading (…)l-00030-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 62.4MB/s]\n",
      "Downloading (…)l-00031-of-00033.bin: 100%|██████████| 405M/405M [00:06<00:00, 61.1MB/s]\n",
      "Downloading (…)l-00032-of-00033.bin: 100%|██████████| 405M/405M [00:08<00:00, 49.7MB/s]\n",
      "Downloading (…)l-00033-of-00033.bin: 100%|██████████| 524M/524M [00:08<00:00, 61.3MB/s]\n",
      "Downloading shards: 100%|██████████| 33/33 [03:54<00:00,  7.11s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.69it/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 124kB/s]\n",
      "Downloading (…)/adapter_config.json: 100%|██████████| 399/399 [00:00<00:00, 159kB/s]\n",
      "Downloading adapter_model.bin: 100%|██████████| 67.2M/67.2M [00:01<00:00, 60.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "\n",
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "LORA_WEIGHTS = \"tloen/alpaca-lora-7b\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, LORA_WEIGHTS, torch_dtype=torch.float16, force_download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device='cuda:0'\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 10.00 GiB total capacity; 9.22 GiB already allocated; 0 bytes free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m      6\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext2text-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     model\u001b[39m=\u001b[39;49mmodel, \n\u001b[0;32m      8\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer, \n\u001b[0;32m      9\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m llm \u001b[39m=\u001b[39m LLMChain(prompt\u001b[39m=\u001b[39mprompt, llm\u001b[39m=\u001b[39mHuggingFacePipeline(pipeline\u001b[39m=\u001b[39mpipe))\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\transformers\\pipelines\\__init__.py:979\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[1;32m--> 979\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline_class(model\u001b[39m=\u001b[39mmodel, framework\u001b[39m=\u001b[39mframework, task\u001b[39m=\u001b[39mtask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:65\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 65\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     67\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_model_type(\n\u001b[0;32m     68\u001b[0m         TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n\u001b[0;32m     69\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[39melse\u001b[39;00m MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n\u001b[0;32m     71\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\transformers\\pipelines\\base.py:773\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m=\u001b[39m framework\n\u001b[0;32m    772\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m--> 773\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m    775\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    776\u001b[0m     \u001b[39m# `accelerate` device map\u001b[39;00m\n\u001b[0;32m    777\u001b[0m     hf_device_map \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mhf_device_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 797 (4 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 10.00 GiB total capacity; 9.22 GiB already allocated; 0 bytes free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "print(f\"{device=}\")\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "llm = LLMChain(prompt=prompt, llm=HuggingFacePipeline(pipeline=pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_directions = \"\"\"\n",
    "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
    "\n",
    "\n",
    "CONSTRAINTS:\n",
    "1. No user assistance.\n",
    "2. Cannot run Python code that requires user input.\n",
    "\n",
    "\n",
    "ACTIONS:\n",
    "\n",
    "1. \"READ_FILE\": read the current state of a file. The schema for the action is:\n",
    "\n",
    "READ_FILE: <PATH>\n",
    "\n",
    "2. \"WRITE_FILE\": write a block of text to a file. The schema for the action is:\n",
    "\n",
    "WRITE_FILE: <PATH>\n",
    "```\n",
    "<TEXT>\n",
    "```\n",
    "\n",
    "3. \"RUN_PYTHON\": run a Python file. The schema for the action is:\n",
    "\n",
    "RUN_PYTHON: <PATH>\n",
    "\n",
    "4. \"SEARCH_ONLINE\": search online and get back a list of URLs relevant to the query. The schema for the action is:\n",
    "\n",
    "SEARCH_ONLINE: <QUERY>\n",
    "\n",
    "5. EXTRACT_INFO: extract specific information from a webpage. The schema for the action is:\n",
    "\n",
    "EXTRACT_INFO: <URL>, <a brief instruction to GPT for information to extract>\n",
    "\n",
    "6. \"SHUTDOWN\": shut down the program. The schema for the action is:\n",
    "\n",
    "SHUTDOWN\n",
    "\n",
    "\n",
    "RESOURCES:\n",
    "1. File contents after reading file.\n",
    "2. Online search results returning URLs.\n",
    "3. Output of running a Python file.\n",
    "\n",
    "\n",
    "PERFORMANCE EVALUATION:\n",
    "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities. \n",
    "2. Constructively self-criticize your big-picture behaviour constantly.\n",
    "3. Reflect on past decisions and strategies to refine your approach.\n",
    "4. Every action has a cost, so be smart and efficent. Aim to complete tasks in the least number of steps.\n",
    "\n",
    "\n",
    "Write only one action. The action must one of the actions specified above and must be written according to the schema specified above.\n",
    "\n",
    "After the action, also write the following metadata JSON object, which must be parsable by Python's json.loads():\n",
    "/{\n",
    "    \"criticism\": \"<constructive self-criticism of actions performed so far, if any>\",\n",
    "    \"reason\": \"<a sentence explaining the action above>\",\n",
    "    \"plan\": \"<a short high-level plan in plain English>\",\n",
    "    \"speak\": \"<a short summary of thoughts to say to the user>\"\n",
    "/}\n",
    "\n",
    "If you want to run an action that is not in the above list of actions, send the SHUTDOWN action instead and explain which action you wanted to run in the metadata JSON object.\n",
    "So, write one action and one metadata JSON object, nothing else.\n",
    "Goal: {goal}\n",
    "AI:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PromptTemplate\n__root__\n  Invalid prompt schema; check for mismatched or missing input parameters. '\\n    \"criticism\"' (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m robo_agent \u001b[39m=\u001b[39m LLMChain(prompt\u001b[39m=\u001b[39mPromptTemplate(template\u001b[39m=\u001b[39;49mgeneral_directions, input_variables\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mgoal\u001b[39;49m\u001b[39m\"\u001b[39;49m]), llm\u001b[39m=\u001b[39mHuggingFacePipeline(pipeline\u001b[39m=\u001b[39mpipe))\n",
      "File \u001b[1;32mc:\\Users\\Aviv9\\.virtualenvs\\Robo-GPT-QaSuWWH8\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for PromptTemplate\n__root__\n  Invalid prompt schema; check for mismatched or missing input parameters. '\\n    \"criticism\"' (type=value_error)"
     ]
    }
   ],
   "source": [
    "robo_agent = LLMChain(prompt=PromptTemplate(template=general_directions, input_variables=[\"goal\"]), llm=HuggingFacePipeline(pipeline=pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'robo_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m robo_agent\u001b[39m.\u001b[39mrun(\u001b[39m\"\u001b[39m\u001b[39mI want you to increase my profile searches in linkedin\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'robo_agent' is not defined"
     ]
    }
   ],
   "source": [
    "robo_agent.run(\"I want you to increase my profile searches in linkedin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robo-GPT-QaSuWWH8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
